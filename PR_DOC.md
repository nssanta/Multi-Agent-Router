# Описание проекта

## Цель проекта

*Цель проекта будет добавлена здесь.*

## Текущая версия

*Версия: 1.0.0*

## Установка и запуск

### Требования

*   Python 3.9+
*   Node.js 18+ и npm

### Backend

1.  **Создайте и активируйте виртуальное окружение:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
2.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```
3.  **Создайте файл `.env`** из примера `.env.example` и заполните необходимые API ключи.
4.  **Запустите сервер:**
    ```bash
    python backend/main.py
    ```
    Сервер будет доступен по адресу `http://localhost:8000`.

### Frontend

1.  **Перейдите в каталог `frontend`:**
    ```bash
    cd frontend
    ```
2.  **Установите зависимости:**
    ```bash
    npm install
    ```
3.  **Запустите сервер для разработки:**
    ```bash
    npm run dev
    ```
    Приложение будет доступно по адресу `http://localhost:5173`.

## Конфигурация

Проект использует переменные окружения для настройки. Создайте файл `.env` в корневом каталоге на основе `.env.example`.

*   `LLM_PROVIDER`: Провайдер LLM. Возможные значения: `gemini` или `openrouter`. По умолчанию `gemini`.
*   `LLM_MODEL`: **Модель LLM по умолчанию**. Используется, если модель не выбрана в интерфейсе. Например, `gemini-2.5-pro` или любая модель, описанная в `backend/models.json`.
*   `GEMINI_API_KEY`: Ваш API ключ для Gemini.
*   `OPENROUTER_API_KEY`: Ваш API ключ для OpenRouter. Для OpenRouter используются только модели, явно перечисленные в `backend/models.json`; роутер `openrouter/auto` не применяется, а в UI отображаются только модели, помеченные тегом `"free"`.
*   `API_HOST`: Хост для запуска FastAPI сервера. По умолчанию `0.0.0.0`.
*   `API_PORT`: Порт для запуска FastAPI сервера. По умолчанию `8000`.

## Архитектура и поток данных

### Обзор

1.  **Пользовательский интерфейс (`frontend`)**: Пользователь взаимодействует с приложением через React-интерфейс. Вверху экрана отображается индикатор использования контекста.
2.  **API (`backend/api`)**: `frontend` отправляет запросы на FastAPI сервер.
3.  **Конфигурация (`backend/core/config.py`, `backend/models.json`)**: При старте сервер загружает информацию о доступных моделях, включая их максимальный размер контекста.
4.  **Управление сессиями (`backend/core/session.py`)**: `SessionManager` создает и управляет сессиями, а также отслеживает общее количество токенов, использованных в каждой сессии.
5.  **Агент (`backend/agents/dialog/agent.py`)**: Для каждой сессии создается агент, который обрабатывает сообщения.
6.  **LLM Провайдер (`backend/core/llm_provider.py`)**: Агент использует LLM для генерации ответов и получает от него данные об использованных токенах.
7.  **Инструменты (`backend/tools/`)**: Агент может использовать инструменты для поиска в интернете, чтения веб-страниц и выполнения кода.

### Поток данных при отправке сообщения

1.  Пользователь в боковой панели (`Sidebar`) выбирает агента и модель (опционально) и создает новую сессию. Выбранная модель сохраняется в `state.model_id` через `SessionManager`.
2.  Пользователь вводит сообщение в `ChatWindow` и нажимает "Отправить".
3.  `frontend/src/services/api.ts` отправляет POST-запрос на эндпоинт `/api/chat` с полями `session_id`, `agent_type`, `message` и `search_enabled`; `model_id` не передается в запросе и берется на бэкенде из состояния сессии.
4.  `backend/api/routes.py` принимает запрос, получает сессию с помощью `SessionManager`, определяет модель по `state.model_id` (или через `get_default_model()` при отсутствии) и создает LLM‑провайдера через `get_llm_provider_for_model`.
5.  Для выбранной модели определяется лимит контекста (через `get_max_context_tokens` и `get_context_limit`), на основе которого LLM‑провайдер оценивает доступное место под содержимое (`calculate_available_space`).
6.  Создается экземпляр `dialog_agent`, которому передаются провайдер LLM и путь к сессии; агент выполняет `run`, формирует промпт (с историей, файлами, ReAct‑инструкциями) и отправляет его в LLM.
7.  Если LLM возвращает команды (например, `SEARCH[...]`, `SMART_SEARCH[...]`, `CONTINUE_SEARCH[...]`), `after_callback` в `dialog_agent` вызывает инструменты (`smart_search`, `web_reader`, `web_search`) и может повторно обратиться к LLM. LLM‑провайдер при этом накапливает usage по всем LLM‑вызовам внутри одного хода.
8.  После завершения хода `routes.py` считывает usage из провайдера (`get_cumulative_usage()`), агрегирует его с накопленными значениями в `state.usage` и формирует структуру `usage` с данными по текущему ходу и всей сессии, а также `model_info`.
9.  `routes.py` сохраняет `usage` и `model_info` в `state` сессии и формирует ответ `{ "response": str, "usage": {...}, "model": {...} }`.
10. `frontend` получает структурированный ответ, отображает текстовое сообщение пользователю, обновляет состояние индикатора использования контекста над чатом (по `usage.context_usage_percent`) и показывает имя активной модели.

## Общее описание структуры проекта

Проект состоит из двух основных частей:

*   **backend**: Серверная часть приложения, написанная на Python. Отвечает за бизнес-логику, обработку запросов и взаимодействие с базой данных.
*   **frontend**: Клиентская часть приложения, разработанная с использованием современных веб-технологий. Отвечает за пользовательский интерфейс и взаимодействие с пользователем.

## Описание каталога `backend`

### Структура каталога

*   `agents/`: Содержит логику для различных агентов.
    *   `dialog/`: Агент для диалогового взаимодействия.
        *   `agent.py`: Основной файл агента.
        *   `prompts.py`: Файл с промптами для агента.
*   `api/`: Отвечает за API и маршрутизацию.
    *   `routes.py`: Определяет маршруты API.
*   `core/`: Ядро приложения.
    *   `agent_framework.py`: Фреймворк для работы с агентами.
    *   `code_executor.py`: Выполняет код.
    *   `config.py`: Загружает и предоставляет конфигурацию моделей.
    *   `llm_provider.py`: Взаимодействует с LLM.
    *   `session.py`: Управляет сессиями.
*   `tools/`: Инструменты, используемые агентами.
    *   `smart_search.py`: Инструмент для умного поиска.
    *   `web_reader.py`: Инструмент для чтения веб-страниц.
    *   `web_search.py`: Инструмент для поиска в интернете.
    *   `file_tools.py`: Инструменты для работы с файлами (read_file, write_file, list_directory, run_code, search_files).
    *   `base.py`: Базовые классы ToolResult, BaseTool, ToolRegistry.
*   `core/tool_calling/`: Универсальная система Tool Calling.
    *   `base.py`: Базовые типы (ToolCall, ToolExecutionResult, ToolCallBatch).
    *   `text_extractor.py`: Многоуровневый парсер для извлечения tool calls из текста LLM.
    *   `native_handler.py`: Обработчик native tool calling через API (OpenAI/OpenRouter формат).
    *   `executor.py`: Универсальный исполнитель инструментов.
*   `main.py`: Основной файл для запуска серверной части.
*   `models.json`: Файл конфигурации для LLM моделей.

### Описание файлов и функций

#### `backend/main.py`

*   **Назначение**: Точка входа для запуска FastAPI приложения.
*   **Логика работы**:
    1.  Загружает переменные окружения из `.env`.
    2.  Настраивает логирование.
    3.  Создает экземпляр FastAPI приложения.
    4.  Подключает роуты из `api.routes`.
    5.  Запускает `uvicorn` сервер.

#### `backend/models.json`

*   **Назначение**: Хранение конфигураций для различных LLM. Позволяет добавлять новые модели и настраивать их параметры (например, `max_context_tokens`), не изменяя код.
*   В конфигурации явно описана модель `gemini-2.5-pro` как дефолтная для провайдера `gemini`; она использует ключ `GEMINI_API_KEY`, имеет лимит контекста ~2M токенов и отображается в UI как "Google Gemini 2.5 Pro (native)".

#### `backend/agents/dialog/agent.py`

*   **Назначение**: Реализация диалогового агента.
*   **Функции**:
    *   `create_dialog_agent(llm_provider, session_path)`:
        *   **Логика работы**:
            1.  Определяет колбэк `after_run` для обработки ответа LLM, вызова инструментов (`SEARCH`, `READ`) и агрегации данных об использованных токенах.
            2.  Создает и возвращает `Agent` с необходимыми параметрами.

#### `backend/agents/dialog/prompts.py`

*   **Назначение**: Хранение строковых констант с промптами для агента.

#### `backend/api/routes.py`

*   **Назначение**: Определение эндпоинтов API.
*   **Функции**:
    *   `startup()`: Выполняется при старте приложения. Создает рабочие директории, загружает конфигурацию моделей из `backend/models.json`, **валидирует, что дефолтная модель для `LLM_PROVIDER`/`LLM_MODEL` существует**, и проверяет наличие соответствующего API-ключа.
    *   `get_models()`: Эндпоинт `GET /api/models`, который возвращает список всех доступных моделей из `models.json` (можно фильтровать по провайдеру через параметр `provider`).
    *   `get_session(agent_type: str, session_id: str)`:
        *   **Возвращает**: данные сессии из `history.json` (включая `messages`, `state`, `created_at`) и путь `path` к директории сессии.
    *   `chat(request: ChatRequest)`:
        *   **Принимает**: `ChatRequest` с полями `session_id`, `agent_type`, `message` и `search_enabled`.
        *   **Поведение**: Определяет модель по `state.model_id` сессии (или дефолтную из конфига), создает LLM‑провайдера через `get_llm_provider_for_model`, сбрасывает счётчики usage, запускает `dialog_agent` с учетом истории, после завершения хода считывает usage из провайдера, агрегирует его с данными сессии и сохраняет новое сообщение в историю.
        *   **Возвращает**: словарь `{ "response": str, "usage": Dict, "model": Dict }`, где `usage` содержит токены за последний ход и за всю сессию, а `model` — информацию об активной модели (id, отображаемое имя, провайдер, лимит контекста).

#### `backend/core/agent_framework.py`

*   **Назначение**: Базовый класс для агентов.
*   **Класс `Agent`**:
    *   `run(message: str, history: list = []) -> str`:
        *   **Возвращает**: Строку с финальным ответом агента.
        *   **Логика работы**:
            1.  Строит промпт на основе глобальной инструкции, истории сообщений и текущего ввода пользователя.
            2.  Вызывает LLM‑провайдера для генерации ответа и, при необходимости, инструменты (по паттернам `SEARCH[...]`, `SMART_SEARCH[...]`, `CONTINUE_SEARCH[...]`) через `after_callback`.
            3.  Возвращает финальный текстовый ответ; учёт токенов осуществляется на стороне LLM‑провайдера и маршрута `/api/chat`.

#### `backend/core/code_executor.py`

*   **Назначение**: Выполнение Python-кода в изолированном окружении.

#### `backend/core/config.py`

*   **Назначение**: Загрузка и предоставление доступа к конфигурациям моделей из `models.json`.
*   **Функции**:
    *   `load_models_config()`: Загружает `backend/models.json` в память, учитывая переменные окружения `LLM_PROVIDER`/`LLM_MODEL` и помечая дефолтные модели.
    *   `get_all_models()`: Возвращает список всех моделей из конфига.
    *   `get_models_for_provider(provider: str)`: Возвращает модели, относящиеся к указанному провайдеру (`gemini`, `openrouter`, `sherlock`, и т.д.).
    *   `get_model_by_id(model_id: str)`: Возвращает конфигурацию конкретной модели или `None`.
    *   `get_default_model(provider: Optional[str] = None)`: Возвращает дефолтную модель для указанного провайдера (или для `LLM_PROVIDER` из `.env`).
    *   `get_max_context_tokens(model_id: str)`: Возвращает максимальный размер контекста для модели (или дефолт 128k, если информация отсутствует).

#### `backend/core/llm_provider.py`

*   **Назначение**: Абстракция для работы с различными LLM и фабрика провайдеров.
*   **Класс `BaseLLMProvider`**:
    *   `generate(prompt: str, **kwargs) -> str`: синхронная генерация текстового ответа.
    *   `stream(prompt: str, **kwargs) -> Iterator[str]`: потоковая генерация ответа.
    *   `get_context_limit()`, `estimate_tokens()`, `calculate_available_space()`: утилиты для оценки и распределения контекста.
    *   `reset_usage()`, `get_cumulative_usage()`, `get_last_usage()`: базовый учёт токенов (prompt/completion/total) за ход для последующей агрегации на уровне сессии.
*   **Классы провайдеров**:
    *   `GeminiProvider`: работа с Google Gemini через `google-generativeai`, получение usage через `response.usage_metadata`.
    *   `OpenRouterProvider`: работа с OpenRouter (OpenAI‑совместимый API); запросы формируются в формате стандартного `chat.completions` без дополнительных пользовательских параметров `usage`, а данные об использовании токенов считываются из поля `usage` в ответе `/chat/completions`.
*   **Фабрики**:
    *   `get_llm_provider(provider_type: str, **config)`: создает провайдера по явному типу (`"gemini"`, `"openrouter"`) и параметрам (ключ, модель).
    *   `get_llm_provider_for_model(model_id: Optional[str] = None)`: создает провайдера на основе записи в `models.json` (если `model_id` не указан — использует дефолтную модель из конфига), автоматически подтягивая нужный API‑ключ.

#### `backend/core/session.py`

*   **Назначение**: Управление сессиями (создание, получение, удаление).
*   **Класс `SessionManager`**:
    *   `create_session(agent_type, user_id, initial_files=None, model_id=None)`: Создает директорию сессии (`input/`, `workspace/`, `logs/`), инициализирует `history.json` и при наличии `model_id` сохраняет его в `state.model_id`.
    *   `get_session(session_id, agent_type)`: Загружает `history.json` и возвращает полную информацию о сессии вместе с путём `path`.
    *   `add_message(...)`: Добавляет сообщение в историю.
    *   `update_state(...)`: Обновляет произвольные поля в `state` (сюда попадают, например, `search_enabled` и `model_id`).
    *   `list_sessions(agent_type=None)`: Возвращает краткий список сессий для боковой панели.
    *   `delete_session(...)`: Удаляет директорию сессии.

#### `backend/tools/*`

*   **Назначение**: Инструменты, используемые агентами (веб-поиск, чтение страниц).

## Описание каталога `frontend`

### Структура каталога

*   `src/`: Исходный код клиентской части.
    *   `components/`: Переиспользуемые компоненты.
        *   `Chat/`: Компоненты, связанные с окном чата (`ChatWindow`, `MessageList`, `MessageInput`).
        *   `Sidebar/`: Компоненты боковой панели (выбор агента и модели, список сессий).
        *   `Browse/`: Компоненты для просмотра сессий и файлов.
    *   `pages/`: Компоненты-страницы (`ChatPage`, `BrowsePage`).
    *   `services/`: Сервисы для взаимодействия с API (`api.ts`).
    *   `types.ts`: Определения типов TypeScript.
*   ... (остальные файлы)

### Описание файлов и компонентов

#### `frontend/src/components/Chat/ChatWindow.tsx`

*   **Назначение**: Окно для отображения и отправки сообщений.
*   **Логика**: Управляет состоянием сообщений, загрузкой истории выбранной сессии и отображением активной модели. При загрузке сессии читает `state.usage` и `state.model_info`, при отправке сообщения вызывает `api.sendMessage`, отображает временный статус "Thinking...", обновляет usage и модель из ответа и рисует над чатом компактный индикатор (имя текущей модели и полоску использования контекста).
*   При получении ошибки `429 Too Many Requests` от бэкенда (как по HTTP‑коду, так и по тексту ответа агента) компонент сохраняет текст последнего пользовательского сообщения и отображает под чатом компактный блок с кнопкой повторной отправки; нажатие запускает до 5 автоматических повторов запроса с небольшими задержками между попытками, при этом сырые технические сообщения об ошибке провайдера (c URL и текстом исключения) не выводятся пользователю, а заменяются на краткие дружелюбные уведомления о rate limit.
*   Кнопка ретрая остаётся доступной до тех пор, пока на это сообщение не будет получен успешный ответ или пока пользователь не начнёт новый чат (сменит сессию), при этом на время выполнения основного запроса или ретрая поле ввода и кнопка отправки блокируются, чтобы пользователь не отправлял новые сообщения до завершения обработки.

#### `frontend/src/services/api.ts`

*   **Назначение**: Клиент для взаимодействия с `backend` API.
*   **Методы**:
    *   `getModels(provider?: string)`: Получает список доступных моделей с эндпоинта `/api/models` (можно фильтровать по провайдеру).
    *   `createSession(agentType: string, userId?: string, modelId?: string)`: Создает новую сессию для выбранного агента и опционально привязывает к ней модель (`model_id`).
    *   `sendMessage(agentType: string, sessionId: string, message: string, searchEnabled?: boolean)`: Отправляет сообщение на бэкенд и возвращает `ChatResponse` (текст ответа, а также при наличии usage и сведения о модели).
    *   `getSession(agentType: string, sessionId: string)`: Возвращает `Promise<SessionHistory>`; объект истории включает массив `messages` и словарь `state`, где хранится, в том числе, `model_id` и другие настройки сессии.

#### `frontend/src/types.ts`

*   **Назначение**: Определения типов TypeScript.
*   **Основные типы**:
    *   `Session`: Краткое описание сессии для списка в боковой панели.
    *   `Message`: Сообщение в истории диалога.
    *   `SessionHistory`: Полная история сессии (`messages`, `state`, `created_at`, и т.д.). Поле `state` представляет собой словарь произвольных значений (в том числе `model_id`, `search_enabled`, агрегированный `usage` и `model_info` для индикатора контекста).
    *   `SessionFile`: Описание файла в сессии.
    *   `Agent`: Описание доступного агента.
    *   `ModelInfo`: Описание LLM‑модели, возвращаемой из `/api/models` (id, `display_name`, `provider`, теги, флаг `is_default`).
    *   `ChatUsage`, `ActiveModelInfo`, `ChatResponse`: структуры, описывающие usage токенов (за ход и за всю сессию), активную модель и ответ эндпоинта `/api/chat`.

## Описание корневых файлов

*   `.env.example`: Пример файла с переменными окружения.
*   `requirements.txt`: Список Python-зависимостей для `backend`.
*   ... (остальные файлы)
