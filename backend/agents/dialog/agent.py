"""
Dialog Agent - –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —á–∞—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å ReAct –ø–æ–¥—Ö–æ–¥–æ–º

–§—É–Ω–∫—Ü–∏–∏:
- –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã
- –í–µ–±-–ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo + –ß–¢–ï–ù–ò–ï —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü
- –ú–æ–∂–µ—Ç —á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª—ã –∏–∑ session/input/
- –ú–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å Python –∫–æ–¥
- ReAct –ø–æ–¥—Ö–æ–¥: Reasoning ‚Üí Acting ‚Üí Observation
"""
from backend.core.agent_framework import Agent
from backend.core.llm_provider import BaseLLMProvider
from backend.core.web_utils import clean_ui_artifacts
from backend.core.code_executor import LocalCodeExecutor
from backend.tools.web_search import duckduckgo_search, format_search_results
from backend.tools.smart_search import smart_search, format_smart_results
from backend.tools.web_reader import read_multiple_urls, format_read_results, smart_chunk_content
from pathlib import Path
import re
import logging
import datetime
from .prompts import DIALOG_INSTRUCTION

logger = logging.getLogger(__name__)


def create_dialog_agent(
    llm_provider: BaseLLMProvider,
    session_path: Path
) -> Agent:
    """–°–æ–∑–¥–∞–µ–º Dialog Agent —Å –≤–µ–±-–ø–æ–∏—Å–∫–æ–º."""
    
    code_executor = LocalCodeExecutor(session_path)

    def get_instruction_with_context(state):
        """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ–º –¥–∞—Ç—É, —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏ —Å—Ç–∞—Ç—É—Å –ø–æ–∏—Å–∫–∞ –≤ –ø—Ä–æ–º–ø—Ç."""
        search_enabled = state.get("search_enabled", True)
        
        # –¢–µ–∫—É—â–∞—è –¥–∞—Ç–∞ –∏ –≤—Ä–µ–º—è UTC
        current_datetime = datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
        
        # –ü–æ–ª—É—á–∏—Ç—å –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º —Å—Ç–∞—Ç—É—Å–∞ –ø–æ–∏—Å–∫–∞
        if search_enabled:
            base_instruction = DIALOG_INSTRUCTION.format(current_datetime=current_datetime)
        else:
            # –ü—Ä–æ–º–ø—Ç –ë–ï–ó –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –ø–æ–∏—Å–∫—É
            base_instruction = f"""You are a helpful AI assistant with advanced capabilities.

**CURRENT DATE AND TIME:** {current_datetime}

**Core Capabilities:**
1. **Answer questions** - provide clear, accurate answers on any topic
2. **Read and analyze files** - work with files in ./input/ directory
3. **Generate and execute Python code** - analyze data, create visualizations, etc.

**NOTE: Web search is currently DISABLED by user. You cannot use SEARCH[] or SMART_SEARCH[] commands.**

**Data Analysis:**
Available Python libraries: pandas, numpy, matplotlib, seaborn, scikit-learn, xgboost, lightgbm

When analyzing data:
1. List available files
2. Load with pandas
3. Perform analysis
4. Show results clearly

**General Guidelines:**
- Be concise and helpful
- **CRITICAL: NEVER make up or hallucinate information!**
- If you don't know something, admit it honestly
- Show code when executing Python

Let's help the user effectively!"""
        
        # –î–æ–±–∞–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
        input_files = list(Path(session_path / "input").glob("*"))
        if input_files:
            files_list = "\n".join([f"- {f.name}" for f in input_files])
            return f"{base_instruction}\n\n**Available Files in Current Session:**\n{files_list}"
        
        return base_instruction

    def before_run(state):
        """–í—ã–ø–æ–ª–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –∞–≥–µ–Ω—Ç–∞."""
        logger.info(f"[DialogAgent] Starting new interaction")
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∫—É—â–∏–π user input –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ after_run
        # (–±—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤ agent_framework.py –ø–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º before_callback)
    
    def after_run(state, response):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è (ITERATIVE multi-turn reasoning):
        1. –í—ã–ø–æ–ª–Ω—è–µ–º –≤–µ–±-–ø–æ–∏—Å–∫ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        2. –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–¢–û–ü-3)
        3. –î–µ–ª–∞–µ–º –í–¢–û–†–û–ô –í–´–ó–û–í LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–≥–æ
        4. –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –µ—â–µ —Ä–∞—É–Ω–¥ - –ø–æ–≤—Ç–æ—Ä—è–µ–º (–¥–æ 3 —Ü–∏–∫–ª–æ–≤)
        5. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
        """
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–∫–ª—é—á–µ–Ω –ª–∏ –ø–æ–∏—Å–∫
        search_enabled = state.get("search_enabled", True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—á–µ—Ç—á–∏–∫ —Ä–∞—É–Ω–¥–æ–≤ (–¥–ª—è iterative reasoning)
        current_round = state.get("reasoning_round", 0)
        
        search_results_text = ""
        read_content_text = ""
        cleaned_response = response
        all_urls_to_read = []
        
        # 1. –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω SMART_SEARCH["query", "target"]
        smart_pattern = r'SMART_SEARCH\["([^"]+)"(?:,\s*"([^"]*)")?\]'
        smart_searches = re.findall(smart_pattern, response)
        
        if smart_searches and not search_enabled:
            # –ü–æ–∏—Å–∫ –æ—Ç–∫–ª—é—á–µ–Ω, –Ω–æ LLM –ø–æ–ø—ã—Ç–∞–ª—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
            logger.warning("[DialogAgent] LLM attempted to use SMART_SEARCH but it's disabled by user")
            cleaned_response = re.sub(smart_pattern, '[Web search is disabled]', response)
        elif smart_searches and search_enabled:
            for query, target in smart_searches:
                target = target.strip() if target else None
                logger.info(f"[DialogAgent] Performing SMART search: query='{query}', target='{target}'")
                
                # –ü–µ—Ä–µ–¥–∞–µ–º llm_provider –¥–ª—è —É–º–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
                result = smart_search(query, target=target, deep=True, llm_provider=llm_provider)
                formatted = format_smart_results(result)
                search_results_text += f"\n\n{formatted}"
                
                # –°–æ–±—Ä–∞—Ç—å URLs –¥–ª—è —á—Ç–µ–Ω–∏—è
                if result.get('results'):
                    urls = [r['url'] for r in result['results'][:3]]  # –¢–û–ü-3
                    all_urls_to_read.extend(urls)
            
            # –£–¥–∞–ª–∏—Ç—å SMART_SEARCH[] –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            cleaned_response = re.sub(smart_pattern, '', response)
        
        # 2. –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –æ–±—ã—á–Ω–æ–≥–æ SEARCH["query"]  
        search_pattern = r'SEARCH\["([^"]+)"\]'
        searches = re.findall(search_pattern, response)
        
        if searches and not search_enabled:
            # –ü–æ–∏—Å–∫ –æ—Ç–∫–ª—é—á–µ–Ω
            logger.warning("[DialogAgent] LLM attempted to use SEARCH but it's disabled by user")
            cleaned_response = re.sub(search_pattern, '[Web search is disabled]', cleaned_response)
        elif searches and search_enabled:
            for query in searches:
                logger.info(f"[DialogAgent] Performing quick search: {query}")
                results = duckduckgo_search(query, max_results=5)
                formatted = format_search_results(results)
                search_results_text += f"\n\n**Quick Search: {query}**\n{formatted}"
                
                # –°–æ–±—Ä–∞—Ç—å URLs –¥–ª—è —á—Ç–µ–Ω–∏—è
                if results:
                    urls = [r['url'] for r in results[:3]]  # –¢–û–ü-3
                    all_urls_to_read.extend(urls)
            
            # –£–¥–∞–ª–∏—Ç—å SEARCH[] –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            cleaned_response = re.sub(search_pattern, '', cleaned_response)
        
        # 3. –ß–ò–¢–ê–¢–¨ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–¢–û–ü-3)
        if all_urls_to_read and search_enabled:
            try:
                logger.info(f"[DialogAgent] Reading content from {len(all_urls_to_read)} URLs...")
                
                # –ß–∏—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ URLs, –º–∞–∫—Å–∏–º—É–º 3
                unique_urls = list(dict.fromkeys(all_urls_to_read))[:3]
                read_results = read_multiple_urls(unique_urls, max_urls=3)
                
                formatted_content = format_read_results(read_results)
                read_content_text = f"\n\n{formatted_content}"
                
                # –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –†–ê–°–ß–ï–¢ –õ–ò–ú–ò–¢–û–í —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
                # –ü–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                user_query = state.get('current_user_input', '')
                instruction = DIALOG_INSTRUCTION.format(current_datetime=datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC"))
                
                # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ–µ –º–µ—Å—Ç–æ (—Å —É—á–µ—Ç–æ–º –ø—Ä–æ–º–ø—Ç–∞, –∏—Å—Ç–æ—Ä–∏–∏, –±—É—Ñ–µ—Ä–∞)
                available_chars = llm_provider.calculate_available_space(
                    system_prompt=instruction,
                    history="",  # –ò—Å—Ç–æ—Ä–∏—è —É–∂–µ —É—á—Ç–µ–Ω–∞ –≤ –ø—Ä–æ–º–ø—Ç–µ
                    buffer_ratio=0.25  # 25% –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –∞–≥–µ–Ω—Ç–∞
                )
                
                # –û—Ü–µ–Ω–∏—Ç—å —Ç–æ–∫–µ–Ω—ã –¥–ª—è –º–µ—Ç—Ä–∏–∫
                estimated_tokens = llm_provider.estimate_tokens(instruction)
                context_limit = llm_provider.get_context_limit()
                
                logger.info(f"[DialogAgent] Context: {estimated_tokens}/{context_limit} tokens used by prompt, {available_chars} chars available for content")
                
                # –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è smart chunking
                query_words = set(re.findall(r'\w+', user_query.lower()))
                
                # –†–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–µ—Å—Ç–æ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ (–ø–æ—Ä–æ–≤–Ω—É)
                num_successful = len([r for r in read_results if r['status'] == 'success'])
                if num_successful > 0:
                    chars_per_page = available_chars // num_successful
                else:
                    chars_per_page = 10000  # Fallback
                
                # –î–æ–±–∞–≤–∏—Ç—å –£–ú–ù–´–ô CHUNKING –¥–ª—è LLM –∞–Ω–∞–ª–∏–∑–∞
                full_content_for_analysis = "\n\n**Page Content:**\n\n"
                total_truncated = 0
                
                for result in read_results:
                    if result['status'] == 'success':
                        main_text = result.get('main_text', '')
                        
                        # –ü—Ä–∏–º–µ–Ω–∏—Ç—å smart chunking
                        chunked = smart_chunk_content(
                            text=main_text,
                            query_words=query_words,
                            max_chars=chars_per_page
                        )
                        
                        full_content_for_analysis += f"**Source: {result['title']}**\n"
                        full_content_for_analysis += f"URL: {result['url']}\n"
                        
                        # –î–æ–±–∞–≤–∏—Ç—å metadata –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω
                        if chunked['truncated']:
                            coverage_percent = int(chunked['coverage'] * 100)
                            full_content_for_analysis += f"*[Showing {coverage_percent}% of content - {chunked['num_paragraphs']} most relevant paragraphs]*\n\n"
                            total_truncated += 1
                        else:
                            full_content_for_analysis += "\n"
                        
                        full_content_for_analysis += f"{chunked['content']}\n\n"
                        full_content_for_analysis += "---\n\n"
                
                read_content_text = clean_ui_artifacts(full_content_for_analysis) + read_content_text
                
                # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                content_tokens = llm_provider.estimate_tokens(full_content_for_analysis)
                total_tokens = estimated_tokens + content_tokens
                usage_percent = (total_tokens / context_limit) * 100
                
                if total_truncated > 0:
                    logger.warning(f"[DialogAgent] {total_truncated}/{num_successful} pages were truncated to fit context")
                
                logger.info(f"[DialogAgent] Successfully read {num_successful} pages (chars: {len(full_content_for_analysis)}, tokens: ~{content_tokens}, total usage: {usage_percent:.1f}%)")
            except Exception as e:
                logger.error(f"[DialogAgent] Error reading URLs: {e}")
                read_content_text = f"\n\n**Note:** Could not read some web pages due to error: {e}"
        
        # 4. –ï—Å–ª–∏ –±—ã–ª –ø–æ–∏—Å–∫ –∏–ª–∏ —á—Ç–µ–Ω–∏–µ - –í–¢–û–†–û–ô –í–´–ó–û–í LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (ITERATIVE)
        if search_results_text or read_content_text:
            current_round += 1
            state.set("reasoning_round", current_round)
            
            logger.info(f"[DialogAgent] Round {current_round}: Performing analysis LLM call...")
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ state (–∞–∫–∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ –¥–ª—è –≤—Å–µ—Ö —Ä–∞—É–Ω–¥–æ–≤)
            prev_searches = state.get("all_search_results", [])
            prev_searches.append({"round": current_round, "search": search_results_text, "content": read_content_text})
            state.set("all_search_results", prev_searches)
            
            # –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å —É—á–µ—Ç–æ–º iterative reasoning
            history_text = ""
            if current_round > 1:
                history_text = f"\n**Previous rounds:**\n"
                for prev in prev_searches[:-1]:
                    history_text += f"Round {prev['round']}: searched and read pages\n"
            
            analysis_prompt = f"""You are analyzing search results. Round {current_round}/3.{history_text}

**Information from current search:**
{read_content_text}

{search_results_text}

**User's question:** {state.get('current_user_input', 'previous question')}

**Your task:**
1. Analyze the information you found
2. Decide: do you have ENOUGH information to answer OR need MORE?
3. If you need MORE specific information: respond with CONTINUE_SEARCH["specific query here"]
4. If you have ENOUGH: provide a clear answer (NO markers like "Final Answer:", just write naturally)

**Guidelines:**
- Use ONLY information from the pages you read
- Cite sources (mention website names)
- Be honest if information is incomplete
- Write conversationally, not with technical markers

**Your response:**"""
            
            # –í—Ç–æ—Ä–æ–π –≤—ã–∑–æ–≤ LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–≥–æ
            try:
                analysis_response = llm_provider.generate(analysis_prompt, temperature=0.7)
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω—É–∂–µ–Ω –ª–∏ –µ—â–µ —Ä–∞—É–Ω–¥ –ø–æ–∏—Å–∫–∞ (–ø–∞—Ç—Ç–µ—Ä–Ω CONTINUE_SEARCH)
                continue_pattern = r'CONTINUE_SEARCH\["([^"]+)"\]'
                continue_searches = re.findall(continue_pattern, analysis_response)
                
                # –ï—Å–ª–∏ –∞–≥–µ–Ω—Ç —Ö–æ—á–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –ò –Ω–µ –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Ä–∞—É–Ω–¥–æ–≤
                if continue_searches and current_round < 3 and search_enabled:
                    logger.info(f"[DialogAgent] Round {current_round}: Agent requests CONTINUE_SEARCH, starting round {current_round + 1}...")
                    
                    # –û—á–∏—Å—Ç–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ –æ—Ç–≤–µ—Ç–∞
                    analysis_response = re.sub(continue_pattern, '', analysis_response).strip()
                    
                    # –í—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–æ–≤—ã–π –ø–æ–∏—Å–∫
                    new_query = continue_searches[0]
                    logger.info(f"[DialogAgent] Round {current_round + 1}: Searching for: {new_query}")
                    
                    new_results = duckduckgo_search(new_query, max_results=5)
                    new_formatted = format_search_results(new_results)
                    
                    # –ü—Ä–æ—á–∏—Ç–∞—Ç—å –Ω–æ–≤—ã–µ URLs —Å —É–º–Ω—ã–º chunking
                    if new_results:
                        new_urls = [r['url'] for r in new_results[:3]]
                        new_read_results = read_multiple_urls(new_urls, max_urls=3)
                        new_read_content = ""
                        
                        # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –ª–∏–º–∏—Ç—ã –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ä–∞—É–Ω–¥–∞
                        num_new_successful = len([r for r in new_read_results if r['status'] == 'success'])
                        if num_new_successful > 0:
                            new_chars_per_page = available_chars // num_new_successful
                        else:
                            new_chars_per_page = 10000
                        
                        for result in new_read_results:
                            if result['status'] == 'success':
                                # –ü—Ä–∏–º–µ–Ω–∏—Ç—å smart chunking
                                new_chunked = smart_chunk_content(
                                    text=result.get('main_text', ''),
                                    query_words=query_words,
                                    max_chars=new_chars_per_page
                                )
                                
                                new_read_content += f"**Source: {result['title']}**\n"
                                new_read_content += f"URL: {result['url']}\n"
                                
                                if new_chunked['truncated']:
                                    coverage = int(new_chunked['coverage'] * 100)
                                    new_read_content += f"*[Showing {coverage}% - {new_chunked['num_paragraphs']} paragraphs]*\n\n"
                                else:
                                    new_read_content += "\n"
                                
                                new_read_content += f"{new_chunked['content']}\n\n---\n\n"
                        
                        # –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ state
                        prev_searches.append({"round": current_round + 1, "search": new_formatted, "content": new_read_content})
                        state.set("all_search_results", prev_searches)
                        state.set("reasoning_round", current_round + 1)
                        
                        # –§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ –Ω–æ–≤–æ–≥–æ —Ä–∞—É–Ω–¥–∞
                        final_prompt = f"""You completed multiple search rounds. Here's ALL the information gathered:

**Previous analysis:**
{analysis_response}

**New search results (round {current_round + 1}):**
{new_read_content}

**User's question:** {state.get('current_user_input')}

**Now provide your FINAL answer:**
- Use all information from all rounds
- Write naturally and conversationally (NO "Final Answer:", "Thought:" markers!)
- Cite sources at the end
- Be clear and helpful

**Your answer:**"""
                        
                        final_response = llm_provider.generate(final_prompt, temperature=0.7)
                        state.set("final_response", final_response)
                        
                        # –î–æ–±–∞–≤–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –∫–æ–Ω–µ—Ü
                        sources_text = "\n\n**üìö Sources:**\n"
                        for prev in prev_searches:
                            urls = re.findall(r'URL: (https?://[^\s]+)', prev['content'])
                            for url in set(urls):
                                sources_text += f"- {url}\n"
                        
                        logger.info(f"[DialogAgent] Iterative reasoning complete after {current_round + 1} rounds")
                        return final_response + sources_text
                
                elif continue_searches and not search_enabled:
                    logger.warning("[DialogAgent] LLM attempted to use CONTINUE_SEARCH but it's disabled by user")
                    analysis_response = re.sub(continue_pattern, '[Web search is disabled]', analysis_response)
                
                # –ï—Å–ª–∏ –Ω–µ –Ω—É–∂–µ–Ω –µ—â–µ —Ä–∞—É–Ω–¥ –ò–õ–ò –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç - —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
                state.set("final_response", analysis_response)
                
                # –î–æ–±–∞–≤–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –∫–æ–Ω–µ—Ü
                sources_text = "\n\n**üìö Sources:**\n"
                for prev in prev_searches:
                    urls = re.findall(r'URL: (https?://[^\s]+)', prev['content'])
                    for url in set(urls):
                        sources_text += f"- {url}\n"
                
                logger.info(f"[DialogAgent] Analysis complete after {current_round} round(s)")
                
                return analysis_response + sources_text
                
            except Exception as e:
                logger.error(f"[DialogAgent] Error in analysis LLM call: {e}")
                # Fallback - –≤–µ—Ä–Ω—É—Ç—å —Ö–æ—Ç—è –±—ã –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                enhanced = clean_ui_artifacts(cleaned_response + search_results_text + read_content_text)
                return enhanced
        
        return clean_ui_artifacts(cleaned_response) if cleaned_response else None

    agent = Agent(
        name="dialog_agent",
        llm_provider=llm_provider,
        instruction=get_instruction_with_context,
        code_executor=code_executor,
        temperature=0.7,
        before_callback=before_run,
        after_callback=after_run
    )
    
    return agent