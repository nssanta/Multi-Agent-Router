"""
Web Content Reader - —á–∏—Ç–∞–µ—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ web_search –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ü–û–õ–ù–û–ì–û —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü,
–∞ –Ω–µ —Ç–æ–ª—å–∫–æ snippets.

Features:
- –ß—Ç–µ–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (—É–±–∏—Ä–∞–µ—Ç –Ω–∞–≤–∏–≥–∞—Ü–∏—é, —Ä–µ–∫–ª–∞–º—É)
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ (timeout, 404, blocked)
- Rate limiting
- Smart chunking —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π –ø–æ keywords
"""

import requests
from bs4 import BeautifulSoup
import time
import logging
from typing import Optional, Dict, List, Set
from urllib.parse import urlparse
import re

logger = logging.getLogger(__name__)

from backend.core.web_utils import clean_ui_artifacts


def smart_chunk_content(
    text: str, 
    query_words: Set[str], 
    max_chars: int,
    paragraph_separator: str = "\n\n"
) -> Dict[str, any]:
    """
    –£–º–Ω—ã–π chunking –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π –ø–æ keywords.
    
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
    1. –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
    2. –û—Ü–µ–Ω–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ (keyword matching)
    3. –í—ã–±—Ä–∞—Ç—å —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –¥–æ max_chars
    4. –í–µ—Ä–Ω—É—Ç—å —Ç–µ–∫—Å—Ç + metadata (% –ø–æ–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
    
    :param text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è chunking
    :param query_words: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    :param max_chars: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤
    :param paragraph_separator: –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
    :return: Dict —Å –ø–æ–ª—è–º–∏ content, coverage, num_paragraphs, truncated
    """
    if len(text) <= max_chars:
        # –¢–µ–∫—Å—Ç –≤–ª–µ–∑–∞–µ—Ç —Ü–µ–ª–∏–∫–æ–º
        return {
            "content": text,
            "coverage": 1.0,
            "num_paragraphs": text.count(paragraph_separator) + 1,
            "truncated": False
        }
    
    # –†–∞–∑–±–∏—Ç—å –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
    paragraphs = text.split(paragraph_separator)
    paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p.strip()) > 20]
    
    if not paragraphs:
        # Fallback: –ø—Ä–æ—Å—Ç–æ –æ–±—Ä–µ–∑–∞—Ç—å —Ç–µ–∫—Å—Ç
        return {
            "content": text[:max_chars] + "...",
            "coverage": max_chars / len(text),
            "num_paragraphs": 1,
            "truncated": True
        }
    
    # –û—Ü–µ–Ω–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
    scored_paragraphs = []
    for para in paragraphs:
        score = _calculate_paragraph_relevance(para, query_words)
        scored_paragraphs.append((score, para))
    
    # –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–±–æ–ª—å—à–µ = –ª—É—á—à–µ)
    scored_paragraphs.sort(reverse=True, key=lambda x: x[0])
    
    # –°–æ–±—Ä–∞—Ç—å —Ç–æ–ø –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –¥–æ max_chars
    selected_paragraphs = []
    current_length = 0
    
    for score, para in scored_paragraphs:
        para_length = len(para) + len(paragraph_separator)
        if current_length + para_length <= max_chars:
            selected_paragraphs.append(para)
            current_length += para_length
        elif current_length < max_chars * 0.8:  # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –º–µ–Ω—å—à–µ 80% - –¥–æ–±–∞–≤–∏—Ç—å —á–∞—Å—Ç–∏—á–Ω–æ
            remaining = max_chars - current_length - 50  # Reserve for "..."
            if remaining > 100:
                selected_paragraphs.append(para[:remaining] + "...")
                current_length = max_chars
                break
        else:
            break
    
    # –°–æ–±—Ä–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    final_content = paragraph_separator.join(selected_paragraphs)
    coverage = len(final_content) / len(text)
    
    return {
        "content": final_content,
        "coverage": coverage,
        "num_paragraphs": len(selected_paragraphs),
        "truncated": True
    }


def _calculate_paragraph_relevance(paragraph: str, query_words: Set[str]) -> float:
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞.
    
    –ö—Ä–∏—Ç–µ—Ä–∏–∏:
    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    - –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (keyword density)
    - –î–ª–∏–Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ (–Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π)
    
    :param paragraph: –¢–µ–∫—Å—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
    :param query_words: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
    :return: –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ)
    """
    score = 0.0
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç
    para_lower = paragraph.lower()
    para_words = set(re.findall(r'\w+', para_lower))
    
    # 1. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö keywords
    matching_words = query_words & para_words
    score += len(matching_words) * 10.0
    
    # 2. –ë–æ–Ω—É—Å –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π –æ–¥–Ω–æ–≥–æ keyword
    for keyword in matching_words:
        occurrences = para_lower.count(keyword)
        if occurrences > 1:
            score += (occurrences - 1) * 2.0
    
    # 3. –î–ª–∏–Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ (–æ–ø—Ç–∏–º—É–º: 200-800 —Å–∏–º–≤–æ–ª–æ–≤)
    para_len = len(paragraph)
    if 200 <= para_len <= 800:
        score += 5.0
    elif para_len > 800:
        score += 2.0  # –î–ª–∏–Ω–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Ç–æ–∂–µ –æ–∫, –Ω–æ –º–µ–Ω–µ–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã
    elif para_len < 100:
        score -= 3.0  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ - —à—Ç—Ä–∞—Ñ
    
    # 4. –ë–æ–Ω—É—Å –µ—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–≥–æ–ª–æ–≤–æ—á–Ω—ã—Ö —Å–ª–æ–≤
    header_indicators = ['introduction', 'overview', 'summary', 'conclusion', 
                        '–≤–≤–µ–¥–µ–Ω–∏–µ', '–æ–±–∑–æ—Ä', '—Ä–µ–∑—é–º–µ', '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ']
    if any(para_lower.startswith(indicator) for indicator in header_indicators):
        score += 3.0
    
    return score


class WebReader:
    """
    –ß–∏—Ç–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç requests + BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç: title, main_text, meta_description.
    """
    
    def __init__(self, timeout: int = 10, rate_limit: float = 1.0):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º WebReader.
        
        :param timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (default: 10)
        :param rate_limit: –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (default: 1.0)
        """
        self.timeout = timeout
        self.rate_limit = rate_limit
        self.last_request_time = 0
        
        # User-Agent –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ—Å—Ç—ã—Ö –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def read_url(self, url: str) -> Dict[str, str]:
        """
        –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ URL.
        
        :param url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —á—Ç–µ–Ω–∏—è
        :return: Dict —Å –ø–æ–ª—è–º–∏: url, title, main_text, meta_description, status, error
        """
        # Rate limiting - –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        elapsed = time.time() - self.last_request_time
        if elapsed < self.rate_limit:
            time.sleep(self.rate_limit - elapsed)
        
        self.last_request_time = time.time()
        
        try:
            logger.info(f"Reading URL: {url}")
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É
            response = requests.get(
                url, 
                headers=self.headers, 
                timeout=self.timeout,
                allow_redirects=True
            )
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–Ω–≥ HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = ""
            if soup.title:
                title = soup.title.string.strip() if soup.title.string else ""
            elif soup.find('h1'):
                title = soup.find('h1').get_text(strip=True)
            
            # –ò–∑–≤–ª–µ—á—å meta description
            meta_desc = ""
            meta_tag = soup.find('meta', attrs={'name': 'description'}) or \
                      soup.find('meta', attrs={'property': 'og:description'})
            if meta_tag and meta_tag.get('content'):
                meta_desc = meta_tag.get('content').strip()
            
            # –ò–∑–≤–ª–µ—á—å –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
            main_text = self._extract_main_text(soup)
            
            # –ù–ï –æ–±—Ä–µ–∑–∞–µ–º –∑–¥–µ—Å—å - —ç—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ –≤ dialog agent —Å —É—á–µ—Ç–æ–º LLM –ª–∏–º–∏—Ç–æ–≤
            # –ù–æ —É—Å—Ç–∞–Ω–æ–≤–∏–º —Ä–∞–∑—É–º–Ω—ã–π –º–∞–∫—Å–∏–º—É–º –Ω–∞ —É—Ä–æ–≤–Ω–µ web reader –¥–ª—è –∑–∞—â–∏—Ç—ã
            original_length = len(main_text)
            max_reader_limit = 50000  # 50K —Å–∏–º–≤–æ–ª–æ–≤ - –∑–∞—â–∏—Ç–∞ –æ—Ç –æ–≥—Ä–æ–º–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            
            if len(main_text) > max_reader_limit:
                main_text = main_text[:max_reader_limit]
                logger.warning(f"Page content exceeds {max_reader_limit} chars, truncated from {original_length}")
            
            logger.info(f"Successfully read {len(main_text)} characters from {url}")
            
            return {
                "url": url,
                "title": title,
                "main_text": main_text,
                "meta_description": meta_desc,
                "status": "success",
                "length": len(main_text)
            }
            
        except requests.exceptions.Timeout:
            logger.warning(f"Timeout reading {url}")
            return {
                "url": url,
                "status": "error",
                "error": "Timeout - page took too long to load"
            }
        
        except requests.exceptions.HTTPError as e:
            logger.warning(f"HTTP error reading {url}: {e}")
            return {
                "url": url,
                "status": "error",
                "error": f"HTTP {response.status_code} - {e}"
            }
        
        except requests.exceptions.RequestException as e:
            logger.warning(f"Request error reading {url}: {e}")
            return {
                "url": url,
                "status": "error",
                "error": f"Network error - {str(e)}"
            }
        
        except Exception as e:
            logger.error(f"Unexpected error reading {url}: {e}")
            return {
                "url": url,
                "status": "error",
                "error": f"Parse error - {str(e)}"
            }
    
    def _extract_main_text(self, soup: BeautifulSoup) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –∏–∑ HTML, —É–±–∏—Ä–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—é –∏ —Ä–µ–∫–ª–∞–º—É.
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
        1. –£–¥–∞–ª–∏—Ç—å –Ω–µ–Ω—É–∂–Ω—ã–µ —Ç–µ–≥–∏ (script, style, nav, footer, ads)
        2. –ò—Å–∫–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ <article>, <main>, –∏–ª–∏ <div class="content">
        3. –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        """
        # –£–¥–∞–ª–∏—Ç—å –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'form']):
            tag.decompose()
        
        # –£–¥–∞–ª–∏—Ç—å —Ä–µ–∫–ª–∞–º–Ω—ã–µ –±–ª–æ–∫–∏ –ø–æ class/id
        for ad_class in ['ad', 'ads', 'advertisement', 'promo', 'sponsored']:
            for element in soup.find_all(class_=lambda x: x and ad_class in x.lower()):
                element.decompose()
            for element in soup.find_all(id=lambda x: x and ad_class in x.lower()):
                element.decompose()
        
        # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        main_content = None
        
        # –í–∞—Ä–∏–∞–Ω—Ç 1: <article> —Ç–µ–≥ (—á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –±–ª–æ–≥–∞—Ö)
        main_content = soup.find('article')
        
        # –í–∞—Ä–∏–∞–Ω—Ç 2: <main> —Ç–µ–≥ (HTML5 semantic)
        if not main_content:
            main_content = soup.find('main')
        
        # –í–∞—Ä–∏–∞–Ω—Ç 3: div —Å –∫–ª–∞—Å—Å–æ–º content/main/post
        if not main_content:
            for class_name in ['content', 'main-content', 'post-content', 'entry-content', 'article-body']:
                main_content = soup.find('div', class_=lambda x: x and class_name in x.lower())
                if main_content:
                    break
        
        # –í–∞—Ä–∏–∞–Ω—Ç 4: –≤–µ—Å—å body (fallback)
        if not main_content:
            main_content = soup.find('body')
        
        if not main_content:
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ - –≤–µ—Ä–Ω—É—Ç—å –≤–µ—Å—å —Ç–µ–∫—Å—Ç
            return soup.get_text(separator=' ', strip=True)
        
        # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, —Å–ø–∏—Å–∫–æ–≤
        text_parts = []
        for element in main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'blockquote']):
            text = element.get_text(strip=True)
            if text and len(text) > 20:  # –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                text_parts.append(text)
        
        # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –Ω–µ –Ω–∞—à–ª–∏ - –≤–∑—è—Ç—å –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        if not text_parts:
            return main_content.get_text(separator=' ', strip=True)
        
        return '\n\n'.join(text_parts)
    
    def read_multiple_urls(self, urls: list, max_urls: int = 3) -> list:
        """
        –ß–∏—Ç–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ URL.
        
        :param urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è —á—Ç–µ–Ω–∏—è
        :param max_urls: –ú–∞–∫—Å–∏–º—É–º URL –¥–ª—è —á—Ç–µ–Ω–∏—è (default: 3)
        :return: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ read_url()
        """
        results = []
        
        for url in urls[:max_urls]:
            result = self.read_url(url)
            results.append(result)
            
            # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ –ø–æ–¥—Ä—è–¥ - –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
            if len(results) >= 2:
                recent_errors = sum(1 for r in results[-2:] if r['status'] == 'error')
                if recent_errors == 2:
                    logger.warning("Too many consecutive errors, stopping reads")
                    break
        
        return results


def format_read_results(results: list) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á—Ç–µ–Ω–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.
    
    :param results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ read_multiple_urls()
    :return: –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
    """
    if not results:
        return "‚ö†Ô∏è **No content read** - Failed to read any URLs."
    
    formatted = f"üìñ **Read {len(results)} pages:**\n\n"
    
    success_count = sum(1 for r in results if r['status'] == 'success')
    error_count = len(results) - success_count
    
    if error_count > 0:
        formatted += f"*(Successfully read {success_count}/{len(results)} pages)*\n\n"
    
    for i, result in enumerate(results, 1):
        if result['status'] == 'error':
            formatted += f"**{i}. ‚ùå {result['url']}**\n"
            formatted += f"   Error: {result['error']}\n\n"
        else:
            title = result.get('title', 'No title')
            text = result.get('main_text', '')
            
            formatted += f"**{i}. ‚úÖ {title}**\n"
            formatted += f"   üîó {result['url']}\n"
            
            # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è preview
            preview = text[:500] + "..." if len(text) > 500 else text
            formatted += f"   üìÑ Content preview:\n   {preview}\n\n"
    
    return clean_ui_artifacts(formatted)


# Singleton instance
_web_reader = None

def get_web_reader() -> WebReader:
    """–ü–æ–ª—É—á–∞–µ–º singleton —ç–∫–∑–µ–º–ø–ª—è—Ä WebReader"""
    global _web_reader
    if _web_reader is None:
        _web_reader = WebReader(timeout=10, rate_limit=1.0)
    return _web_reader


def read_url(url: str) -> Dict[str, str]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —á—Ç–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ URL.
    :param url: URL
    :return: —Ä–µ–∑—É–ª—å—Ç–∞—Ç —á—Ç–µ–Ω–∏—è
    """
    reader = get_web_reader()
    return reader.read_url(url)


def read_multiple_urls(urls: list, max_urls: int = 3) -> list:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —á—Ç–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö URL.
    :param urls: —Å–ø–∏—Å–æ–∫ URL
    :param max_urls: –ª–∏–º–∏—Ç
    :return: —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    reader = get_web_reader()
    return reader.read_multiple_urls(urls, max_urls)
