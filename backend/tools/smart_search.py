"""
Smart Multi-Step Search Tool
–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –ë–ï–ó API keys, —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ DuckDuckGo
"""

from typing import List, Dict, Optional
import logging
import hashlib
import time
import re
from .web_search import duckduckgo_search

logger = logging.getLogger(__name__)

from backend.core.web_utils import clean_ui_artifacts

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–µ—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ (–∂–∏–≤–µ—Ç –≤ —Ä–∞–º–∫–∞—Ö —Å–µ—Å—Å–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è)
_search_cache = {}
_cache_max_age = 600  # 10 –º–∏–Ω—É—Ç


class SmartSearch:
    """
    –£–º–Ω—ã–π –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π –ø–æ–∏—Å–∫–æ–≤–∏–∫
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –¶–µ–ª–µ–≤–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Å–∞–π—Ç–∞–º (GitHub, StackOverflow, Reddit)
    - –ú–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π –ø–æ–∏—Å–∫ (–µ—Å–ª–∏ –ø–µ—Ä–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–ª–æ—Ö–∏–µ - –∏—â–µ—Ç –ø–æ-–¥—Ä—É–≥–æ–º—É)
    - –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    - LLM-driven –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ (–∞–≥–µ–Ω—Ç –°–ê–ú –¥—É–º–∞–µ—Ç –∫–∞–∫ –∏—Å–∫–∞—Ç—å)
    """
    
    def __init__(self, llm_provider=None):
        self.max_steps = 5  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 5 —Ä–∞—É–Ω–¥–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
        self.results_per_step = 5
        self.llm_provider = llm_provider  # –î–ª—è —É–º–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        self.use_cache = True  # –í–∫–ª—é—á–∏—Ç—å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    
    def _get_cache_key(self, query: str, target: Optional[str]) -> str:
        """–°–æ–∑–¥–∞—Ç—å –∫–ª—é—á –∫–µ—à–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        cache_str = f"{query.lower()}:{target or 'none'}"
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def _get_from_cache(self, query: str, target: Optional[str]) -> Optional[List[Dict]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –∫–µ—à–∞ –µ—Å–ª–∏ –µ—Å—Ç—å –∏ –Ω–µ —É—Å—Ç–∞—Ä–µ–ª–∏"""
        if not self.use_cache:
            return None
        
        cache_key = self._get_cache_key(query, target)
        if cache_key in _search_cache:
            cached_data = _search_cache[cache_key]
            age = time.time() - cached_data['timestamp']
            
            if age < _cache_max_age:
                logger.info(f"Cache HIT for query '{query[:30]}...' (age: {age:.1f}s)")
                return cached_data['results']
            else:
                # –£—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫–µ—à - —É–¥–∞–ª–∏—Ç—å
                del _search_cache[cache_key]
                logger.debug(f"Cache expired for query '{query[:30]}...'")
        
        return None
    
    def _save_to_cache(self, query: str, target: Optional[str], results: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–µ—à"""
        if not self.use_cache:
            return
        
        cache_key = self._get_cache_key(query, target)
        _search_cache[cache_key] = {
            'results': results,
            'timestamp': time.time()
        }
        logger.debug(f"Saved to cache: query '{query[:30]}...'")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∫–µ—à–∞ (–º–∞–∫—Å–∏–º—É–º 100 –∑–∞–ø–∏—Å–µ–π)
        if len(_search_cache) > 100:
            # –£–¥–∞–ª–∏—Ç—å —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å
            oldest_key = min(_search_cache.keys(), key=lambda k: _search_cache[k]['timestamp'])
            del _search_cache[oldest_key]
            logger.debug("Cache size limit reached, removed oldest entry")
    
    def search(
        self,
        query: str,
        target: Optional[str] = None,
        deep: bool = True
    ) -> Dict:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å —É–º–Ω—ã–π –ø–æ–∏—Å–∫
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            target: –¶–µ–ª–µ–≤–æ–π —Å–∞–π—Ç ("github", "stackoverflow", "reddit", None)
            deep: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞—É–Ω–¥–æ–≤)
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
                - results: List[Dict] - –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                - steps: int - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –ø–æ–∏—Å–∫–∞
                - queries: List[str] - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        """
        logger.info(f"SmartSearch starting: query='{query}', target={target}, deep={deep}")
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–µ—à —Å–Ω–∞—á–∞–ª–∞
        cached_results = self._get_from_cache(query, target)
        if cached_results is not None:
            return {
                "results": cached_results[:10],
                "steps": 0,
                "queries": [query],
                "total_found": len(cached_results),
                "from_cache": True
            }
        
        all_results = []
        queries_used = []
        seen_urls = set()
        step = 0
        
        # –®–∞–≥ 1: –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫
        primary_query = self._build_query(query, target)
        queries_used.append(primary_query)
        step += 1
        
        results = duckduckgo_search(primary_query, max_results=self.results_per_step)
        for r in results:
            if r.get('url') not in seen_urls:
                all_results.append(r)
                seen_urls.add(r.get('url'))
        
        logger.info(f"Step {step}: found {len(results)} results (unique: {len(all_results)})")
        
        # –ï—Å–ª–∏ deep —Ä–µ–∂–∏–º - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏—Å–∫–∞—Ç—å –¥–æ max_steps –∏–ª–∏ –ø–æ–∫–∞ –Ω–µ –Ω–∞–±–µ—Ä–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
        if deep and step < self.max_steps:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å
            unique_count = len(all_results)
            target_results = 7  # –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
            iterations_without_new_results = 0  # –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è
            max_empty_iterations = 2  # –ú–∞–∫—Å–∏–º—É–º 2 –∏—Ç–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ –Ω–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
            while unique_count < target_results and step < self.max_steps and iterations_without_new_results < max_empty_iterations:
                results_before = unique_count
                
                # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤
                if self.llm_provider and step >= 2:
                    # –° —à–∞–≥–∞ 3+ –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É–º–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
                    new_queries = self._generate_query_variants(query, target, all_results)
                else:
                    # –ü–µ—Ä–≤—ã–µ 2 —à–∞–≥–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º rule-based (–±—ã—Å—Ç—Ä–µ–µ)
                    if step == 1:
                        new_queries = [self._reformulate_query(query, target)]
                    elif step == 2 and target:
                        # –£–±—Ä–∞—Ç—å site: —Ñ–∏–ª—å—Ç—Ä –¥–ª—è –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
                        new_queries = [query]
                    else:
                        break  # –ù–µ—Ç –±–æ–ª—å—à–µ –ø—Ä–∞–≤–∏–ª
                
                # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                for new_query in new_queries:
                    if new_query in queries_used:
                        continue  # –£–∂–µ –ø—Ä–æ–±–æ–≤–∞–ª–∏
                    
                    if step >= self.max_steps:
                        break
                    
                    queries_used.append(new_query)
                    step += 1
                    
                    new_results = duckduckgo_search(new_query, max_results=self.results_per_step)
                    added = 0
                    for r in new_results:
                        if r.get('url') not in seen_urls:
                            all_results.append(r)
                            seen_urls.add(r.get('url'))
                            added += 1
                    
                    logger.debug(f"Step {step}: query='{new_query[:50]}...', found {len(new_results)} ({added} new unique)")
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–±–∞–≤–∏–ª–∏—Å—å –ª–∏ –Ω–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                unique_count = len(all_results)
                if unique_count == results_before:
                    iterations_without_new_results += 1
                    logger.debug(f"No new results added, iteration {iterations_without_new_results}/{max_empty_iterations}")
                else:
                    iterations_without_new_results = 0
        
        # –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        ranked_results = self._rank_results(all_results, query, target)
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫–µ—à
        self._save_to_cache(query, target, ranked_results[:10])
        
        logger.info(f"SmartSearch complete: {len(ranked_results)} unique results from {step} steps")
        
        return {
            "results": ranked_results[:10],  # –¢–æ–ø-10
            "steps": step,
            "queries": queries_used,
            "total_found": len(all_results),
            "from_cache": False
        }
    
    def _build_query(self, query: str, target: Optional[str]) -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ü–µ–ª–µ–≤–æ–π –∑–∞–ø—Ä–æ—Å"""
        if not target:
            return query
        
        # –¶–µ–ª–µ–≤—ã–µ —Å–∞–π—Ç—ã
        site_map = {
            "github": "site:github.com",
            "stackoverflow": "site:stackoverflow.com",
            "reddit": "site:reddit.com",
            "arxiv": "site:arxiv.org",
            "medium": "site:medium.com",
            "docs": "site:readthedocs.io OR site:docs.python.org"
        }
        
        site_filter = site_map.get(target.lower(), "")
        if site_filter:
            return f"{query} {site_filter}"
        
        return query
    
    def _generate_query_variants(
        self,
        original_query: str,
        target: Optional[str],
        current_results: List[Dict]
    ) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–∞ –∏—Å–ø–æ–ª—å–∑—É—è LLM
        
        LLM –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∏ —Ç–µ–∫—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã,
        –∑–∞—Ç–µ–º –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç 2-3 –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
        """
        if not self.llm_provider:
            # Fallback –Ω–∞ rule-based –µ—Å–ª–∏ –Ω–µ—Ç LLM
            return [self._reformulate_query(original_query, target)]
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
            results_preview = ""
            if current_results:
                results_preview = "Current results found:\n"
                for i, r in enumerate(current_results[:3], 1):
                    results_preview += f"{i}. {r.get('title', 'No title')}\n"
            else:
                results_preview = "No results found yet."
            
            target_hint = ""
            if target:
                target_hint = f"\nTarget site: {target}"
            
            # –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM —Å JSON —Ñ–æ—Ä–º–∞—Ç–æ–º (–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥)
            prompt = f"""Generate 2-3 alternative search queries to find better results.

Original query: "{original_query}"{target_hint}

{results_preview}

Task: Create alternative search queries that:
- Use different keywords or synonyms
- Try different angles or aspects
- Are specific and focused
- Would work well in DuckDuckGo search

Respond with ONLY valid JSON array of strings, nothing else.
Example format:
["alternative query one", "another search variation", "third search option"]"""

            # –ü–æ–ª—É—á–∏—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç LLM
            response = self.llm_provider.generate(prompt, temperature=0.7)
            
            # –ü–∞—Ä—Å–∏—Ç—å JSON –æ—Ç–≤–µ—Ç
            variants = []
            try:
                import json
                # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–µ—Ä–Ω—É—Ç –≤ ```json```)
                json_match = re.search(r'\[.*\]', response, re.DOTALL)
                if json_match:
                    parsed = json.loads(json_match.group(0))
                    if isinstance(parsed, list):
                        for item in parsed[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3
                            if isinstance(item, str) and len(item.strip()) > 3:
                                query = item.strip()
                                # –î–æ–±–∞–≤–∏—Ç—å site: —Ñ–∏–ª—å—Ç—Ä –µ—Å–ª–∏ –Ω—É–∂–µ–Ω
                                if target:
                                    query = self._build_query(query, target)
                                variants.append(query)
                        
                        if variants:
                            logger.info(f"LLM generated {len(variants)} query variants (JSON format)")
                            return variants[:3]
            except (json.JSONDecodeError, AttributeError) as e:
                logger.warning(f"Failed to parse JSON from LLM, trying fallback parsing: {e}")
            
            # Fallback: plain text –ø–∞—Ä—Å–∏–Ω–≥ –µ—Å–ª–∏ JSON –Ω–µ —É–¥–∞–ª—Å—è
            for line in response.strip().split('\n'):
                line = line.strip()
                # –£–±—Ä–∞—Ç—å markdown code blocks
                if line.startswith('```') or line.startswith('[') or line.startswith(']'):
                    continue
                # –£–±—Ä–∞—Ç—å –Ω—É–º–µ—Ä–∞—Ü–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å
                if line and not line.startswith('#'):
                    # –£–±—Ä–∞—Ç—å "1. ", "- ", –∫–∞–≤—ã—á–∫–∏ –∏ —Ç.–¥.
                    cleaned = re.sub(r'^[\d\.\-\)\s"]+', '', line).rstrip('",')
                    if cleaned and len(cleaned) > 3 and not cleaned.startswith('{'):
                        # –î–æ–±–∞–≤–∏—Ç—å site: —Ñ–∏–ª—å—Ç—Ä –µ—Å–ª–∏ –Ω—É–∂–µ–Ω
                        if target:
                            cleaned = self._build_query(cleaned, target)
                        variants.append(cleaned)
            
            if variants:
                logger.info(f"LLM generated {len(variants)} query variants")
                return variants[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞
            
        except Exception as e:
            logger.error(f"Error generating query variants with LLM: {e}")
        
        # Fallback –Ω–∞ rule-based
        return [self._reformulate_query(original_query, target)]
    
    def _reformulate_query(self, query: str, target: Optional[str]) -> str:
        """
        –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (rule-based fallback)
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏–∏:
        - –î–æ–±–∞–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        - –£–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞
        - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏–Ω–æ–Ω–∏–º—ã
        """
        # –î–ª—è GitHub: –¥–æ–±–∞–≤–∏—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        if target == "github":
            if "repository" not in query.lower() and "repo" not in query.lower():
                query = f"{query} repository"
            if "implementation" not in query.lower():
                query = f"{query} implementation"
        
        # –î–ª—è StackOverflow: –¥–æ–±–∞–≤–∏—Ç—å "how to" –∏–ª–∏ "tutorial"
        elif target == "stackoverflow":
            if not any(word in query.lower() for word in ["how", "tutorial", "example"]):
                query = f"how to {query}"
        
        # –î–ª—è Reddit: –¥–æ–±–∞–≤–∏—Ç—å "discussion" –∏–ª–∏ "best"
        elif target == "reddit":
            if "discussion" not in query.lower():
                query = f"{query} discussion"
        
        return self._build_query(query, target)
    
    def _deduplicate(self, results: List[Dict]) -> List[Dict]:
        """–£–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL"""
        seen_urls = set()
        unique = []
        
        for result in results:
            url = result.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique.append(result)
        
        return unique
    
    def _rank_results(
        self,
        results: List[Dict],
        query: str,
        target: Optional[str]
    ) -> List[Dict]:
        """
        –†–∞–Ω–∂–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        
        –ö—Ä–∏—Ç–µ—Ä–∏–∏:
        - –ù–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ title/snippet
        - –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Å–∞–π—Ç–∞
        - –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞—Ç–∞)
        """
        query_words = set(query.lower().split())
        
        def calculate_score(result: Dict) -> float:
            score = 0.0
            
            title = result.get('title', '').lower()
            snippet = result.get('snippet', '').lower()
            url = result.get('url', '').lower()
            
            # –ë–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
            title_words = set(title.split())
            matching_words = query_words & title_words
            score += len(matching_words) * 2.0
            
            # –ë–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
            snippet_words = set(snippet.split())
            matching_snippet = query_words & snippet_words
            score += len(matching_snippet) * 1.0
            
            # –ë–æ–Ω—É—Å –∑–∞ —Ü–µ–ª–µ–≤–æ–π —Å–∞–π—Ç
            if target:
                site_map = {
                    "github": "github.com",
                    "stackoverflow": "stackoverflow.com",
                    "reddit": "reddit.com"
                }
                if site_map.get(target, "") in url:
                    score += 5.0
            
            # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–∞–π—Ç—ã
            if "github.com" in url:
                score += 3.0
            if "stackoverflow.com" in url:
                score += 2.5
            if "medium.com" in url or "towardsdatascience.com" in url:
                score += 2.0
            if "arxiv.org" in url:
                score += 3.5
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ —Ä–µ–∫–ª–∞–º–Ω—ã–µ —Å–∞–π—Ç—ã
            spam_indicators = ["ad", "promo", "buy", "shop"]
            if any(spam in url for spam in spam_indicators):
                score -= 5.0
            
            return score
        
        # –î–æ–±–∞–≤–∏—Ç—å score –∫ –∫–∞–∂–¥–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        for result in results:
            result['relevance_score'] = calculate_score(result)
        
        # –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ score (–±–æ–ª—å—à–µ = –ª—É—á—à–µ)
        return sorted(results, key=lambda x: x.get('relevance_score', 0), reverse=True)


def smart_search(
    query: str,
    target: Optional[str] = None,
    deep: bool = True,
    llm_provider=None
) -> Dict:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        target: –¶–µ–ª–µ–≤–æ–π —Å–∞–π—Ç (github, stackoverflow, reddit, None)
        deep: –ì–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞—É–Ω–¥–æ–≤)
        llm_provider: LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —É–º–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    
    Example:
        # –ü–æ–∏—Å–∫ –Ω–∞ GitHub
        results = smart_search("reinforcement learning blackjack", target="github")
        
        # –û–±—â–∏–π –ø–æ–∏—Å–∫ —Å —É–≥–ª—É–±–ª–µ–Ω–∏–µ–º
        results = smart_search("best practices python async", deep=True)
        
        # Stack Overflow –ø–æ–∏—Å–∫
        results = smart_search("how to use asyncio", target="stackoverflow")
        
        # –° LLM –¥–ª—è —É–º–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        results = smart_search("RL blackjack", target="github", llm_provider=llm)
    """
    searcher = SmartSearch(llm_provider=llm_provider)
    return searcher.search(query, target, deep)


def format_smart_results(search_result: Dict) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã SmartSearch –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    
    Args:
        search_result: Dict –∏–∑ smart_search()
    
    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
    """
    results = search_result.get('results', [])
    steps = search_result.get('steps', 0)
    queries = search_result.get('queries', [])
    total_found = search_result.get('total_found', 0)
    
    if not results:
        return "‚ö†Ô∏è **No results found** - Smart search tried multiple queries but found nothing. Cannot provide information on this topic."
    
    formatted = f"üîç **Smart Search Results** (Found {len(results)} unique results from {total_found} total, {steps} steps):\n\n"
    
    # –ü–æ–∫–∞–∑–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ
    if len(queries) > 1:
        formatted += "üìä *Search strategy:*\n"
        for i, q in enumerate(queries, 1):
            formatted += f"  Step {i}: `{q}`\n"
        formatted += "\n"
    
    # –¢–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for i, result in enumerate(results[:7], 1):  # –¢–æ–ø-7
        score = result.get('relevance_score', 0)
        title = result.get('title', 'No title')
        snippet = result.get('snippet', 'No description')
        url = result.get('url', '')
        
        # –≠–º–æ–¥–∑–∏ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevance_badge = ""
        if score >= 5:
            relevance_badge = " üî•"
        elif score >= 3:
            relevance_badge = " ‚≠ê"
        
        formatted += f"{i}. **{title}**{relevance_badge}\n"
        formatted += f"   üìé {url}\n"
        formatted += f"   üìù {snippet[:180]}{'...' if len(snippet) > 180 else ''}\n\n"
    
    return clean_ui_artifacts(formatted)
